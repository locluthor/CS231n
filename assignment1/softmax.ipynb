{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.322715\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** \n",
    "\n",
    "*Because all the weights that we randomly created is too small so the scores for every class is really close to each others. So for every example the probability of all classes is the same as each other and about 0.1. So the loss of every example in the training set will be close to -ln(0.1). This is why we expect our loss to be close to -log(0.1) with out initial W.*\n",
    "\n",
    "*Note that this is not true if the all the values in the initial Weights is not close to zero*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -1.294407 analytic: -1.294408, relative error: 3.390923e-08\n",
      "numerical: -1.084011 analytic: -1.084011, relative error: 3.224277e-08\n",
      "numerical: 1.606201 analytic: 1.606201, relative error: 4.852503e-08\n",
      "numerical: 0.596364 analytic: 0.596364, relative error: 1.590744e-08\n",
      "numerical: -0.803802 analytic: -0.803802, relative error: 3.267585e-09\n",
      "numerical: -1.163895 analytic: -1.163895, relative error: 2.247237e-08\n",
      "numerical: -2.120570 analytic: -2.120570, relative error: 2.519209e-08\n",
      "numerical: -0.159102 analytic: -0.159102, relative error: 2.868624e-07\n",
      "numerical: 2.511925 analytic: 2.511924, relative error: 1.672853e-08\n",
      "numerical: 2.175882 analytic: 2.175882, relative error: 1.758838e-10\n",
      "numerical: 1.687364 analytic: 1.687364, relative error: 1.995376e-08\n",
      "numerical: -1.656660 analytic: -1.656660, relative error: 1.814831e-08\n",
      "numerical: -1.448016 analytic: -1.448016, relative error: 1.438480e-08\n",
      "numerical: 1.823814 analytic: 1.823814, relative error: 6.426952e-09\n",
      "numerical: -3.329272 analytic: -3.329272, relative error: 2.472737e-08\n",
      "numerical: -1.111506 analytic: -1.111506, relative error: 4.368861e-09\n",
      "numerical: 2.130548 analytic: 2.130548, relative error: 5.175244e-08\n",
      "numerical: 0.848829 analytic: 0.848829, relative error: 9.133887e-08\n",
      "numerical: 0.426168 analytic: 0.426168, relative error: 1.838564e-07\n",
      "numerical: -0.647879 analytic: -0.647879, relative error: 3.725777e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.322715e+00 computed in 0.089235s\n",
      "vectorized loss: 2.322715e+00 computed in 0.006018s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing lr, rs  (7.443982983985744e-06, 8837.981683174208)\n",
      "iteration 0 / 500: loss 279.910760\n",
      "iteration 100 / 500: loss 2.948022\n",
      "iteration 200 / 500: loss 2.926531\n",
      "iteration 300 / 500: loss 2.821345\n",
      "iteration 400 / 500: loss 3.828572\n",
      "Testing lr, rs  (2.407223122824098e-06, 38643.04992353442)\n",
      "iteration 0 / 500: loss 3.599118\n",
      "iteration 100 / 500: loss 2.109330\n",
      "iteration 200 / 500: loss 2.169720\n",
      "iteration 300 / 500: loss 2.222492\n",
      "iteration 400 / 500: loss 2.165694\n",
      "Testing lr, rs  (8.60165336276023e-06, 5439.766843450076)\n",
      "iteration 0 / 500: loss 2.038494\n",
      "iteration 100 / 500: loss 4.334983\n",
      "iteration 200 / 500: loss 3.223109\n",
      "iteration 300 / 500: loss 4.214067\n",
      "iteration 400 / 500: loss 3.657076\n",
      "Testing lr, rs  (7.910184066590607e-06, 59482.30114432842)\n",
      "iteration 0 / 500: loss 6.900592\n",
      "iteration 100 / 500: loss 11.185205\n",
      "iteration 200 / 500: loss 10.854379\n",
      "iteration 300 / 500: loss 12.378227\n",
      "iteration 400 / 500: loss 12.343914\n",
      "Testing lr, rs  (9.728360786415645e-06, 5011.213217294676)\n",
      "iteration 0 / 500: loss 8.337134\n",
      "iteration 100 / 500: loss 4.038455\n",
      "iteration 200 / 500: loss 3.551247\n",
      "iteration 300 / 500: loss 3.984273\n",
      "iteration 400 / 500: loss 3.669127\n",
      "Testing lr, rs  (3.1692016081652807e-06, 98214.30446662476)\n",
      "iteration 0 / 500: loss 14.397559\n",
      "iteration 100 / 500: loss 2.347635\n",
      "iteration 200 / 500: loss 2.437620\n",
      "iteration 300 / 500: loss 2.300615\n",
      "iteration 400 / 500: loss 2.607222\n",
      "Testing lr, rs  (2.9284415040611003e-06, 21856.208966502785)\n",
      "iteration 0 / 500: loss 2.182501\n",
      "iteration 100 / 500: loss 2.153797\n",
      "iteration 200 / 500: loss 2.072624\n",
      "iteration 300 / 500: loss 2.124106\n",
      "iteration 400 / 500: loss 2.078396\n",
      "Testing lr, rs  (6.459039624753077e-06, 12408.376752297409)\n",
      "iteration 0 / 500: loss 2.061457\n",
      "iteration 100 / 500: loss 3.971739\n",
      "iteration 200 / 500: loss 2.729866\n",
      "iteration 300 / 500: loss 2.612582\n",
      "iteration 400 / 500: loss 4.086596\n",
      "Testing lr, rs  (1.438873467596221e-06, 56363.87249358497)\n",
      "iteration 0 / 500: loss 4.303604\n",
      "iteration 100 / 500: loss 2.201557\n",
      "iteration 200 / 500: loss 2.183104\n",
      "iteration 300 / 500: loss 2.149899\n",
      "iteration 400 / 500: loss 2.131243\n",
      "Testing lr, rs  (2.799722011290587e-06, 26538.64411333764)\n",
      "iteration 0 / 500: loss 2.120436\n",
      "iteration 100 / 500: loss 2.091841\n",
      "iteration 200 / 500: loss 2.114991\n",
      "iteration 300 / 500: loss 2.124608\n",
      "iteration 400 / 500: loss 2.071188\n",
      "Testing lr, rs  (8.558156363993754e-06, 40764.00265364677)\n",
      "iteration 0 / 500: loss 2.271635\n",
      "iteration 100 / 500: loss 10.833169\n",
      "iteration 200 / 500: loss 10.040360\n",
      "iteration 300 / 500: loss 10.271305\n",
      "iteration 400 / 500: loss 9.021956\n",
      "Testing lr, rs  (9.041059342307644e-06, 92583.40088356732)\n",
      "iteration 0 / 500: loss 14.534030\n",
      "iteration 100 / 500: loss 113.718568\n",
      "iteration 200 / 500: loss 124.461677\n",
      "iteration 300 / 500: loss 120.111838\n",
      "iteration 400 / 500: loss 124.531157\n",
      "Testing lr, rs  (9.332825578297628e-07, 36758.449659600265)\n",
      "iteration 0 / 500: loss 63.654253\n",
      "iteration 100 / 500: loss 2.054427\n",
      "iteration 200 / 500: loss 2.124036\n",
      "iteration 300 / 500: loss 2.136599\n",
      "iteration 400 / 500: loss 2.160788\n",
      "Testing lr, rs  (5.652278037434707e-06, 23378.388961250526)\n",
      "iteration 0 / 500: loss 2.133298\n",
      "iteration 100 / 500: loss 2.669999\n",
      "iteration 200 / 500: loss 2.790573\n",
      "iteration 300 / 500: loss 2.721099\n",
      "iteration 400 / 500: loss 3.116958\n",
      "Testing lr, rs  (9.849275374969417e-07, 51893.200730215474)\n",
      "iteration 0 / 500: loss 4.345945\n",
      "iteration 100 / 500: loss 2.192000\n",
      "iteration 200 / 500: loss 2.173122\n",
      "iteration 300 / 500: loss 2.188942\n",
      "iteration 400 / 500: loss 2.122092\n",
      "Testing lr, rs  (5.708235332542443e-06, 524.3796736678394)\n",
      "iteration 0 / 500: loss 2.104146\n",
      "iteration 100 / 500: loss 2.007579\n",
      "iteration 200 / 500: loss 2.004899\n",
      "iteration 300 / 500: loss 1.873337\n",
      "iteration 400 / 500: loss 2.314388\n",
      "Testing lr, rs  (1.58665052671006e-06, 65785.45240068679)\n",
      "iteration 0 / 500: loss 10.890958\n",
      "iteration 100 / 500: loss 2.145817\n",
      "iteration 200 / 500: loss 2.201795\n",
      "iteration 300 / 500: loss 2.224037\n",
      "iteration 400 / 500: loss 2.202671\n",
      "Testing lr, rs  (7.032190665251946e-06, 2010.1120743824947)\n",
      "iteration 0 / 500: loss 2.103880\n",
      "iteration 100 / 500: loss 2.899226\n",
      "iteration 200 / 500: loss 3.178313\n",
      "iteration 300 / 500: loss 2.972991\n",
      "iteration 400 / 500: loss 3.204888\n",
      "Testing lr, rs  (9.130559334597836e-06, 411.6992012153702)\n",
      "iteration 0 / 500: loss 2.838302\n",
      "iteration 100 / 500: loss 2.664705\n",
      "iteration 200 / 500: loss 2.258504\n",
      "iteration 300 / 500: loss 2.088988\n",
      "iteration 400 / 500: loss 2.845656\n",
      "Testing lr, rs  (6.5255230236712065e-06, 13847.58971501177)\n",
      "iteration 0 / 500: loss 6.321171\n",
      "iteration 100 / 500: loss 3.742691\n",
      "iteration 200 / 500: loss 3.321413\n",
      "iteration 300 / 500: loss 3.141085\n",
      "iteration 400 / 500: loss 2.732870\n",
      "Testing lr, rs  (1.1580601831726663e-08, 82460.93460448139)\n",
      "iteration 0 / 500: loss 5.156411\n",
      "iteration 100 / 500: loss 3.587772\n",
      "iteration 200 / 500: loss 2.809347\n",
      "iteration 300 / 500: loss 2.422929\n",
      "iteration 400 / 500: loss 2.400137\n",
      "Testing lr, rs  (8.304241969307018e-06, 65619.14697580953)\n",
      "iteration 0 / 500: loss 2.231783\n",
      "iteration 100 / 500: loss 19.705891\n",
      "iteration 200 / 500: loss 20.539043\n",
      "iteration 300 / 500: loss 18.366222\n",
      "iteration 400 / 500: loss 17.932267\n",
      "Testing lr, rs  (7.188251998887862e-06, 75418.92968481155)\n",
      "iteration 0 / 500: loss 17.774921\n",
      "iteration 100 / 500: loss 13.133996\n",
      "iteration 200 / 500: loss 12.955395\n",
      "iteration 300 / 500: loss 16.127043\n",
      "iteration 400 / 500: loss 13.605205\n",
      "Testing lr, rs  (8.00475662823067e-06, 12588.307654589837)\n",
      "iteration 0 / 500: loss 8.466347\n",
      "iteration 100 / 500: loss 3.400275\n",
      "iteration 200 / 500: loss 4.759825\n",
      "iteration 300 / 500: loss 4.158595\n",
      "iteration 400 / 500: loss 4.691899\n",
      "Testing lr, rs  (5.376531439274513e-06, 29315.47339093188)\n",
      "iteration 0 / 500: loss 4.148572\n",
      "iteration 100 / 500: loss 2.987749\n",
      "iteration 200 / 500: loss 3.044899\n",
      "iteration 300 / 500: loss 2.653442\n",
      "iteration 400 / 500: loss 3.027157\n",
      "Testing lr, rs  (8.062641909626919e-06, 35460.41684709216)\n",
      "iteration 0 / 500: loss 2.952859\n",
      "iteration 100 / 500: loss 6.401928\n",
      "iteration 200 / 500: loss 6.713941\n",
      "iteration 300 / 500: loss 5.462519\n",
      "iteration 400 / 500: loss 6.241968\n",
      "Testing lr, rs  (5.268511288660229e-06, 46907.16446691123)\n",
      "iteration 0 / 500: loss 6.793545\n",
      "iteration 100 / 500: loss 3.589815\n",
      "iteration 200 / 500: loss 2.999967\n",
      "iteration 300 / 500: loss 4.414201\n",
      "iteration 400 / 500: loss 3.494259\n",
      "Testing lr, rs  (4.639464312462098e-06, 55879.83469146094)\n",
      "iteration 0 / 500: loss 3.630378\n",
      "iteration 100 / 500: loss 4.049313\n",
      "iteration 200 / 500: loss 2.730011\n",
      "iteration 300 / 500: loss 3.821575\n",
      "iteration 400 / 500: loss 3.592117\n",
      "Testing lr, rs  (4.030652785162529e-06, 98790.703099393)\n",
      "iteration 0 / 500: loss 5.010981\n",
      "iteration 100 / 500: loss 3.765974\n",
      "iteration 200 / 500: loss 4.393434\n",
      "iteration 300 / 500: loss 3.390822\n",
      "iteration 400 / 500: loss 5.625460\n",
      "Testing lr, rs  (2.9638021222671656e-07, 66739.00708494536)\n",
      "iteration 0 / 500: loss 4.238045\n",
      "iteration 100 / 500: loss 2.124447\n",
      "iteration 200 / 500: loss 2.175728\n",
      "iteration 300 / 500: loss 2.089806\n",
      "iteration 400 / 500: loss 2.207430\n",
      "Testing lr, rs  (5.30527376452636e-06, 64575.800336057335)\n",
      "iteration 0 / 500: loss 2.149806\n",
      "iteration 100 / 500: loss 4.935715\n",
      "iteration 200 / 500: loss 4.190838\n",
      "iteration 300 / 500: loss 3.922274\n",
      "iteration 400 / 500: loss 5.780404\n",
      "Testing lr, rs  (5.673793932142294e-06, 77533.21280586424)\n",
      "iteration 0 / 500: loss 5.054000\n",
      "iteration 100 / 500: loss 7.443351\n",
      "iteration 200 / 500: loss 7.116387\n",
      "iteration 300 / 500: loss 7.171043\n",
      "iteration 400 / 500: loss 5.787381\n",
      "Testing lr, rs  (5.0081975994694e-06, 23951.48157769985)\n",
      "iteration 0 / 500: loss 5.653227\n",
      "iteration 100 / 500: loss 2.350827\n",
      "iteration 200 / 500: loss 2.716687\n",
      "iteration 300 / 500: loss 2.366923\n",
      "iteration 400 / 500: loss 2.625307\n",
      "Testing lr, rs  (8.693497838841472e-06, 73423.93677598941)\n",
      "iteration 0 / 500: loss 3.544802\n",
      "iteration 100 / 500: loss 26.129000\n",
      "iteration 200 / 500: loss 27.679835\n",
      "iteration 300 / 500: loss 26.108527\n",
      "iteration 400 / 500: loss 25.772006\n",
      "Testing lr, rs  (4.069785260571668e-06, 54862.136327993525)\n",
      "iteration 0 / 500: loss 22.079504\n",
      "iteration 100 / 500: loss 2.677883\n",
      "iteration 200 / 500: loss 2.986366\n",
      "iteration 300 / 500: loss 2.485449\n",
      "iteration 400 / 500: loss 2.654774\n",
      "Testing lr, rs  (9.7858659929021e-06, 16000.848197860787)\n",
      "iteration 0 / 500: loss 2.790524\n",
      "iteration 100 / 500: loss 8.652561\n",
      "iteration 200 / 500: loss 5.052666\n",
      "iteration 300 / 500: loss 6.734224\n",
      "iteration 400 / 500: loss 7.337580\n",
      "Testing lr, rs  (8.194367166466697e-07, 26391.88117524315)\n",
      "iteration 0 / 500: loss 6.720781\n",
      "iteration 100 / 500: loss 2.085445\n",
      "iteration 200 / 500: loss 2.079992\n",
      "iteration 300 / 500: loss 2.121617\n",
      "iteration 400 / 500: loss 2.075303\n",
      "Testing lr, rs  (4.3663964709076775e-06, 93206.56596592424)\n",
      "iteration 0 / 500: loss 2.343219\n",
      "iteration 100 / 500: loss 5.774081\n",
      "iteration 200 / 500: loss 4.959474\n",
      "iteration 300 / 500: loss 5.169829\n",
      "iteration 400 / 500: loss 5.321448\n",
      "Testing lr, rs  (2.340618394895386e-06, 73225.24547897726)\n",
      "iteration 0 / 500: loss 3.625038\n",
      "iteration 100 / 500: loss 2.268588\n",
      "iteration 200 / 500: loss 2.227223\n",
      "iteration 300 / 500: loss 2.232268\n",
      "iteration 400 / 500: loss 2.180600\n",
      "Testing lr, rs  (5.750295830892542e-06, 1514.2669707012485)\n",
      "iteration 0 / 500: loss 2.127148\n",
      "iteration 100 / 500: loss 2.131529\n",
      "iteration 200 / 500: loss 2.026428\n",
      "iteration 300 / 500: loss 2.021830\n",
      "iteration 400 / 500: loss 1.962519\n",
      "Testing lr, rs  (9.022349561302567e-07, 79271.75977968896)\n",
      "iteration 0 / 500: loss 6.734245\n",
      "iteration 100 / 500: loss 2.144234\n",
      "iteration 200 / 500: loss 2.189507\n",
      "iteration 300 / 500: loss 2.166425\n",
      "iteration 400 / 500: loss 2.181508\n",
      "Testing lr, rs  (7.53057534968859e-07, 97196.0592354649)\n",
      "iteration 0 / 500: loss 2.247532\n",
      "iteration 100 / 500: loss 2.206440\n",
      "iteration 200 / 500: loss 2.227109\n",
      "iteration 300 / 500: loss 2.207193\n",
      "iteration 400 / 500: loss 2.197921\n",
      "Testing lr, rs  (1.1297057627062774e-06, 58447.8171054072)\n",
      "iteration 0 / 500: loss 2.164549\n",
      "iteration 100 / 500: loss 2.128996\n",
      "iteration 200 / 500: loss 2.195677\n",
      "iteration 300 / 500: loss 2.175097\n",
      "iteration 400 / 500: loss 2.158531\n",
      "Testing lr, rs  (6.266189082231417e-06, 90133.69516477478)\n",
      "iteration 0 / 500: loss 2.222682\n",
      "iteration 100 / 500: loss 11.766911\n",
      "iteration 200 / 500: loss 12.821358\n",
      "iteration 300 / 500: loss 12.210871\n",
      "iteration 400 / 500: loss 11.329540\n",
      "Testing lr, rs  (5.968208640721161e-06, 1280.5429579074137)\n",
      "iteration 0 / 500: loss 8.707689\n",
      "iteration 100 / 500: loss 2.357337\n",
      "iteration 200 / 500: loss 2.041312\n",
      "iteration 300 / 500: loss 2.071130\n",
      "iteration 400 / 500: loss 1.906901\n",
      "Testing lr, rs  (1.550728315099694e-06, 71311.74896927094)\n",
      "iteration 0 / 500: loss 7.921337\n",
      "iteration 100 / 500: loss 2.228330\n",
      "iteration 200 / 500: loss 2.193522\n",
      "iteration 300 / 500: loss 2.197373\n",
      "iteration 400 / 500: loss 2.193501\n",
      "Testing lr, rs  (9.02858490066875e-06, 40901.891819162054)\n",
      "iteration 0 / 500: loss 2.177206\n",
      "iteration 100 / 500: loss 10.532009\n",
      "iteration 200 / 500: loss 11.711282\n",
      "iteration 300 / 500: loss 9.407082\n",
      "iteration 400 / 500: loss 10.328838\n",
      "Testing lr, rs  (8.6277193091384e-06, 61867.30749193482)\n",
      "iteration 0 / 500: loss 12.128785\n",
      "iteration 100 / 500: loss 15.111327\n",
      "iteration 200 / 500: loss 16.331695\n",
      "iteration 300 / 500: loss 16.022340\n",
      "iteration 400 / 500: loss 15.912115\n",
      "Testing lr, rs  (9.30104944096827e-06, 67203.44817331462)\n",
      "iteration 0 / 500: loss 20.692442\n",
      "iteration 100 / 500: loss 26.941548\n",
      "iteration 200 / 500: loss 26.019586\n",
      "iteration 300 / 500: loss 23.466582\n",
      "iteration 400 / 500: loss 29.787563\n",
      "Testing lr, rs  (8.670273596432644e-07, 7287.491718674322)\n",
      "iteration 0 / 500: loss 14.190933\n",
      "iteration 100 / 500: loss 2.005821\n",
      "iteration 200 / 500: loss 2.013486\n",
      "iteration 300 / 500: loss 2.006990\n",
      "iteration 400 / 500: loss 2.022124\n",
      "Testing lr, rs  (1.52824253465347e-06, 79023.07180586731)\n",
      "iteration 0 / 500: loss 2.880612\n",
      "iteration 100 / 500: loss 2.204362\n",
      "iteration 200 / 500: loss 2.161289\n",
      "iteration 300 / 500: loss 2.239810\n",
      "iteration 400 / 500: loss 2.209140\n",
      "Testing lr, rs  (8.401333842160718e-06, 97679.59514526444)\n",
      "iteration 0 / 500: loss 2.237251\n",
      "iteration 100 / 500: loss 82.864179\n",
      "iteration 200 / 500: loss 87.371511\n",
      "iteration 300 / 500: loss 91.626401\n",
      "iteration 400 / 500: loss 93.866920\n",
      "Testing lr, rs  (4.087375483460241e-06, 99273.02383805753)\n",
      "iteration 0 / 500: loss 97.685666\n",
      "iteration 100 / 500: loss 4.917709\n",
      "iteration 200 / 500: loss 4.056917\n",
      "iteration 300 / 500: loss 4.156337\n",
      "iteration 400 / 500: loss 4.070455\n",
      "Testing lr, rs  (1.8205392650671843e-06, 23833.275885102314)\n",
      "iteration 0 / 500: loss 4.542745\n",
      "iteration 100 / 500: loss 2.202312\n",
      "iteration 200 / 500: loss 2.122168\n",
      "iteration 300 / 500: loss 2.149749\n",
      "iteration 400 / 500: loss 2.112275\n",
      "Testing lr, rs  (9.929891842222615e-06, 41848.80805513454)\n",
      "iteration 0 / 500: loss 2.205870\n",
      "iteration 100 / 500: loss 13.746930\n",
      "iteration 200 / 500: loss 13.581350\n",
      "iteration 300 / 500: loss 12.110747\n",
      "iteration 400 / 500: loss 13.286619\n",
      "Testing lr, rs  (1.2597271473791583e-06, 17545.8351462285)\n",
      "iteration 0 / 500: loss 9.945213\n",
      "iteration 100 / 500: loss 2.043279\n",
      "iteration 200 / 500: loss 2.075333\n",
      "iteration 300 / 500: loss 2.095184\n",
      "iteration 400 / 500: loss 2.151789\n",
      "Testing lr, rs  (6.07458421707996e-06, 61998.26150321958)\n",
      "iteration 0 / 500: loss 2.316785\n",
      "iteration 100 / 500: loss 5.987834\n",
      "iteration 200 / 500: loss 5.750200\n",
      "iteration 300 / 500: loss 7.823036\n",
      "iteration 400 / 500: loss 6.582573\n",
      "Testing lr, rs  (7.589849855844309e-07, 92663.15051534046)\n",
      "iteration 0 / 500: loss 9.276268\n",
      "iteration 100 / 500: loss 2.176329\n",
      "iteration 200 / 500: loss 2.223643\n",
      "iteration 300 / 500: loss 2.192457\n",
      "iteration 400 / 500: loss 2.189178\n",
      "Testing lr, rs  (2.0014093703075584e-06, 77989.82923198807)\n",
      "iteration 0 / 500: loss 2.203544\n",
      "iteration 100 / 500: loss 2.205373\n",
      "iteration 200 / 500: loss 2.188398\n",
      "iteration 300 / 500: loss 2.253209\n",
      "iteration 400 / 500: loss 2.196983\n",
      "Testing lr, rs  (4.0403474499252655e-06, 54839.1227611999)\n",
      "iteration 0 / 500: loss 2.162733\n",
      "iteration 100 / 500: loss 2.747029\n",
      "iteration 200 / 500: loss 2.391859\n",
      "iteration 300 / 500: loss 2.448690\n",
      "iteration 400 / 500: loss 2.544327\n",
      "Testing lr, rs  (1.4420175953932038e-06, 64776.65254975934)\n",
      "iteration 0 / 500: loss 2.672535\n",
      "iteration 100 / 500: loss 2.193012\n",
      "iteration 200 / 500: loss 2.172974\n",
      "iteration 300 / 500: loss 2.170032\n",
      "iteration 400 / 500: loss 2.182153\n",
      "Testing lr, rs  (6.403823738477458e-06, 42804.305639163926)\n",
      "iteration 0 / 500: loss 2.115100\n",
      "iteration 100 / 500: loss 4.884200\n",
      "iteration 200 / 500: loss 5.344933\n",
      "iteration 300 / 500: loss 4.500518\n",
      "iteration 400 / 500: loss 5.505532\n",
      "Testing lr, rs  (5.572491365716491e-06, 38653.68759765224)\n",
      "iteration 0 / 500: loss 5.060738\n",
      "iteration 100 / 500: loss 3.895254\n",
      "iteration 200 / 500: loss 2.924464\n",
      "iteration 300 / 500: loss 3.515804\n",
      "iteration 400 / 500: loss 3.166869\n",
      "Testing lr, rs  (6.319291837731262e-06, 28868.068273330544)\n",
      "iteration 0 / 500: loss 4.601252\n",
      "iteration 100 / 500: loss 3.088481\n",
      "iteration 200 / 500: loss 3.847694\n",
      "iteration 300 / 500: loss 3.382760\n",
      "iteration 400 / 500: loss 4.445863\n",
      "Testing lr, rs  (1.2908555258077241e-07, 70510.3355250167)\n",
      "iteration 0 / 500: loss 3.513231\n",
      "iteration 100 / 500: loss 2.177561\n",
      "iteration 200 / 500: loss 2.208715\n",
      "iteration 300 / 500: loss 2.183466\n",
      "iteration 400 / 500: loss 2.207011\n",
      "Testing lr, rs  (8.424165837725408e-06, 52693.0489111805)\n",
      "iteration 0 / 500: loss 2.141212\n",
      "iteration 100 / 500: loss 12.620024\n",
      "iteration 200 / 500: loss 11.764505\n",
      "iteration 300 / 500: loss 13.094406\n",
      "iteration 400 / 500: loss 12.520411\n",
      "Testing lr, rs  (2.7126270874677623e-06, 15950.739157810738)\n",
      "iteration 0 / 500: loss 9.634009\n",
      "iteration 100 / 500: loss 2.132055\n",
      "iteration 200 / 500: loss 2.044922\n",
      "iteration 300 / 500: loss 2.092013\n",
      "iteration 400 / 500: loss 2.056712\n",
      "Testing lr, rs  (5.765357141062589e-06, 63601.7602258286)\n",
      "iteration 0 / 500: loss 2.376899\n",
      "iteration 100 / 500: loss 4.855370\n",
      "iteration 200 / 500: loss 5.792220\n",
      "iteration 300 / 500: loss 6.210723\n",
      "iteration 400 / 500: loss 7.292118\n",
      "Testing lr, rs  (7.899934341615879e-06, 33330.55502826886)\n",
      "iteration 0 / 500: loss 6.894816\n",
      "iteration 100 / 500: loss 5.839518\n",
      "iteration 200 / 500: loss 8.401417\n",
      "iteration 300 / 500: loss 5.107590\n",
      "iteration 400 / 500: loss 7.473437\n",
      "Testing lr, rs  (4.802781798450288e-06, 64022.64605169857)\n",
      "iteration 0 / 500: loss 9.609061\n",
      "iteration 100 / 500: loss 5.163991\n",
      "iteration 200 / 500: loss 3.313078\n",
      "iteration 300 / 500: loss 3.173084\n",
      "iteration 400 / 500: loss 4.593036\n",
      "Testing lr, rs  (2.174914034406244e-06, 84708.1611348946)\n",
      "iteration 0 / 500: loss 4.262130\n",
      "iteration 100 / 500: loss 2.229829\n",
      "iteration 200 / 500: loss 2.233338\n",
      "iteration 300 / 500: loss 2.186501\n",
      "iteration 400 / 500: loss 2.222232\n",
      "Testing lr, rs  (9.35634747631649e-07, 36292.33469711559)\n",
      "iteration 0 / 500: loss 2.178104\n",
      "iteration 100 / 500: loss 2.143680\n",
      "iteration 200 / 500: loss 2.086150\n",
      "iteration 300 / 500: loss 2.182201\n",
      "iteration 400 / 500: loss 2.122367\n",
      "Testing lr, rs  (4.756741148944775e-06, 41358.92542216141)\n",
      "iteration 0 / 500: loss 2.170792\n",
      "iteration 100 / 500: loss 3.019024\n",
      "iteration 200 / 500: loss 2.538162\n",
      "iteration 300 / 500: loss 3.116662\n",
      "iteration 400 / 500: loss 3.435122\n",
      "Testing lr, rs  (1.2828254144787814e-06, 72641.95293655337)\n",
      "iteration 0 / 500: loss 3.126475\n",
      "iteration 100 / 500: loss 2.236606\n",
      "iteration 200 / 500: loss 2.244916\n",
      "iteration 300 / 500: loss 2.179076\n",
      "iteration 400 / 500: loss 2.177362\n",
      "Testing lr, rs  (1.3227596190844043e-06, 39348.77694002355)\n",
      "iteration 0 / 500: loss 2.116794\n",
      "iteration 100 / 500: loss 2.139179\n",
      "iteration 200 / 500: loss 2.167995\n",
      "iteration 300 / 500: loss 2.163507\n",
      "iteration 400 / 500: loss 2.129192\n",
      "Testing lr, rs  (7.091108590418655e-06, 74892.53557928346)\n",
      "iteration 0 / 500: loss 2.214857\n",
      "iteration 100 / 500: loss 12.316832\n",
      "iteration 200 / 500: loss 13.683223\n",
      "iteration 300 / 500: loss 12.690849\n",
      "iteration 400 / 500: loss 10.885956\n",
      "Testing lr, rs  (8.629151118391553e-06, 30610.978757082852)\n",
      "iteration 0 / 500: loss 10.328218\n",
      "iteration 100 / 500: loss 6.783608\n",
      "iteration 200 / 500: loss 8.679523\n",
      "iteration 300 / 500: loss 8.541389\n",
      "iteration 400 / 500: loss 9.206495\n",
      "Testing lr, rs  (2.344393775461355e-06, 39436.785983635535)\n",
      "iteration 0 / 500: loss 6.978397\n",
      "iteration 100 / 500: loss 2.147866\n",
      "iteration 200 / 500: loss 2.116959\n",
      "iteration 300 / 500: loss 2.223907\n",
      "iteration 400 / 500: loss 2.112626\n",
      "Testing lr, rs  (2.932465817867001e-07, 93308.32951159708)\n",
      "iteration 0 / 500: loss 2.301558\n",
      "iteration 100 / 500: loss 2.187449\n",
      "iteration 200 / 500: loss 2.178386\n",
      "iteration 300 / 500: loss 2.173919\n",
      "iteration 400 / 500: loss 2.190655\n",
      "Testing lr, rs  (9.017951040574203e-06, 49872.098480595116)\n",
      "iteration 0 / 500: loss 2.169804\n",
      "iteration 100 / 500: loss 11.326046\n",
      "iteration 200 / 500: loss 12.005538\n",
      "iteration 300 / 500: loss 11.302482\n",
      "iteration 400 / 500: loss 9.914964\n",
      "Testing lr, rs  (7.188137414760048e-06, 57417.59916152917)\n",
      "iteration 0 / 500: loss 12.987211\n",
      "iteration 100 / 500: loss 6.665656\n",
      "iteration 200 / 500: loss 8.838305\n",
      "iteration 300 / 500: loss 10.439130\n",
      "iteration 400 / 500: loss 9.435147\n",
      "Testing lr, rs  (7.502336816876877e-07, 33760.02393312858)\n",
      "iteration 0 / 500: loss 7.448300\n",
      "iteration 100 / 500: loss 2.079532\n",
      "iteration 200 / 500: loss 2.107405\n",
      "iteration 300 / 500: loss 2.125157\n",
      "iteration 400 / 500: loss 2.081146\n",
      "Testing lr, rs  (6.865156790613755e-06, 62100.51388512362)\n",
      "iteration 0 / 500: loss 2.178858\n",
      "iteration 100 / 500: loss 9.034090\n",
      "iteration 200 / 500: loss 9.027139\n",
      "iteration 300 / 500: loss 6.912125\n",
      "iteration 400 / 500: loss 10.122775\n",
      "Testing lr, rs  (1.9623702743608754e-06, 38502.7436085665)\n",
      "iteration 0 / 500: loss 7.554577\n",
      "iteration 100 / 500: loss 2.181688\n",
      "iteration 200 / 500: loss 2.162729\n",
      "iteration 300 / 500: loss 2.146567\n",
      "iteration 400 / 500: loss 2.127862\n",
      "Testing lr, rs  (5.985508436759704e-06, 73781.976043042)\n",
      "iteration 0 / 500: loss 2.243772\n",
      "iteration 100 / 500: loss 8.804210\n",
      "iteration 200 / 500: loss 8.574220\n",
      "iteration 300 / 500: loss 8.389682\n",
      "iteration 400 / 500: loss 9.618337\n",
      "Testing lr, rs  (6.077761053901414e-06, 78819.76752328704)\n",
      "iteration 0 / 500: loss 11.183693\n",
      "iteration 100 / 500: loss 11.563788\n",
      "iteration 200 / 500: loss 10.055419\n",
      "iteration 300 / 500: loss 9.338582\n",
      "iteration 400 / 500: loss 10.271528\n",
      "Testing lr, rs  (3.306772047462431e-06, 59999.75709228674)\n",
      "iteration 0 / 500: loss 8.344518\n",
      "iteration 100 / 500: loss 2.226485\n",
      "iteration 200 / 500: loss 2.378698\n",
      "iteration 300 / 500: loss 2.218846\n",
      "iteration 400 / 500: loss 2.173850\n",
      "Testing lr, rs  (8.858745313646675e-06, 6678.242787014389)\n",
      "iteration 0 / 500: loss 2.169547\n",
      "iteration 100 / 500: loss 3.032880\n",
      "iteration 200 / 500: loss 3.030238\n",
      "iteration 300 / 500: loss 4.520203\n",
      "iteration 400 / 500: loss 3.809553\n",
      "Testing lr, rs  (3.7374422800149963e-06, 98347.0704229161)\n",
      "iteration 0 / 500: loss 8.888968\n",
      "iteration 100 / 500: loss 3.256581\n",
      "iteration 200 / 500: loss 3.187457\n",
      "iteration 300 / 500: loss 2.740945\n",
      "iteration 400 / 500: loss 2.840660\n",
      "Testing lr, rs  (4.777906569495198e-06, 53272.39513235151)\n",
      "iteration 0 / 500: loss 3.356911\n",
      "iteration 100 / 500: loss 3.755058\n",
      "iteration 200 / 500: loss 3.758174\n",
      "iteration 300 / 500: loss 4.428532\n",
      "iteration 400 / 500: loss 4.288960\n",
      "Testing lr, rs  (7.946270593042883e-06, 91553.69323766223)\n",
      "iteration 0 / 500: loss 3.293226\n",
      "iteration 100 / 500: loss 40.063894\n",
      "iteration 200 / 500: loss 37.135606\n",
      "iteration 300 / 500: loss 38.797442\n",
      "iteration 400 / 500: loss 42.820564\n",
      "Testing lr, rs  (2.7897259044875785e-06, 5906.831075241512)\n",
      "iteration 0 / 500: loss 17.340957\n",
      "iteration 100 / 500: loss 1.942347\n",
      "iteration 200 / 500: loss 2.034882\n",
      "iteration 300 / 500: loss 2.003226\n",
      "iteration 400 / 500: loss 2.031135\n",
      "Testing lr, rs  (8.01253117130313e-06, 63696.688843614036)\n",
      "iteration 0 / 500: loss 2.960578\n",
      "iteration 100 / 500: loss 14.334005\n",
      "iteration 200 / 500: loss 14.181224\n",
      "iteration 300 / 500: loss 14.450006\n",
      "iteration 400 / 500: loss 15.025888\n",
      "Testing lr, rs  (7.748477796777496e-06, 34264.67003539268)\n",
      "iteration 0 / 500: loss 11.424699\n",
      "iteration 100 / 500: loss 6.198316\n",
      "iteration 200 / 500: loss 6.501759\n",
      "iteration 300 / 500: loss 6.306561\n",
      "iteration 400 / 500: loss 6.856347\n",
      "Testing lr, rs  (3.735778083426712e-06, 22498.213862888017)\n",
      "iteration 0 / 500: loss 5.271259\n",
      "iteration 100 / 500: loss 2.276909\n",
      "iteration 200 / 500: loss 2.161061\n",
      "iteration 300 / 500: loss 2.175357\n",
      "iteration 400 / 500: loss 2.209949\n",
      "Testing lr, rs  (4.157746520832239e-06, 20652.703564650128)\n",
      "iteration 0 / 500: loss 2.210701\n",
      "iteration 100 / 500: loss 2.236748\n",
      "iteration 200 / 500: loss 2.222326\n",
      "iteration 300 / 500: loss 2.218894\n",
      "iteration 400 / 500: loss 2.127388\n",
      "Testing lr, rs  (8.103055961038555e-06, 23762.697658371286)\n",
      "iteration 0 / 500: loss 2.157644\n",
      "iteration 100 / 500: loss 6.416159\n",
      "iteration 200 / 500: loss 5.963601\n",
      "iteration 300 / 500: loss 4.523985\n",
      "iteration 400 / 500: loss 5.603200\n",
      "Testing lr, rs  (2.89921846030924e-06, 6974.001027565118)\n",
      "iteration 0 / 500: loss 2.942805\n",
      "iteration 100 / 500: loss 2.002654\n",
      "iteration 200 / 500: loss 2.014941\n",
      "iteration 300 / 500: loss 2.048080\n",
      "iteration 400 / 500: loss 1.905976\n",
      "Testing lr, rs  (5.448149165307194e-06, 71344.68079767714)\n",
      "iteration 0 / 500: loss 2.875955\n",
      "iteration 100 / 500: loss 5.986231\n",
      "iteration 200 / 500: loss 5.771151\n",
      "iteration 300 / 500: loss 5.993882\n",
      "iteration 400 / 500: loss 6.024356\n",
      "Testing lr, rs  (4.448980889561207e-06, 24198.216305612706)\n",
      "iteration 0 / 500: loss 5.303859\n",
      "iteration 100 / 500: loss 2.223569\n",
      "iteration 200 / 500: loss 2.283876\n",
      "iteration 300 / 500: loss 2.219104\n",
      "iteration 400 / 500: loss 2.161102\n",
      "lr 1.158060e-08 reg 8.246093e+04 train accuracy: 0.303286 val accuracy: 0.318000\n",
      "lr 1.290856e-07 reg 7.051034e+04 train accuracy: 0.306531 val accuracy: 0.322000\n",
      "lr 2.932466e-07 reg 9.330833e+04 train accuracy: 0.286367 val accuracy: 0.299000\n",
      "lr 2.963802e-07 reg 6.673901e+04 train accuracy: 0.297367 val accuracy: 0.312000\n",
      "lr 7.502337e-07 reg 3.376002e+04 train accuracy: 0.311694 val accuracy: 0.334000\n",
      "lr 7.530575e-07 reg 9.719606e+04 train accuracy: 0.284878 val accuracy: 0.291000\n",
      "lr 7.589850e-07 reg 9.266315e+04 train accuracy: 0.271000 val accuracy: 0.283000\n",
      "lr 8.194367e-07 reg 2.639188e+04 train accuracy: 0.323612 val accuracy: 0.341000\n",
      "lr 8.670274e-07 reg 7.287492e+03 train accuracy: 0.357184 val accuracy: 0.353000\n",
      "lr 9.022350e-07 reg 7.927176e+04 train accuracy: 0.288939 val accuracy: 0.306000\n",
      "lr 9.332826e-07 reg 3.675845e+04 train accuracy: 0.316551 val accuracy: 0.327000\n",
      "lr 9.356347e-07 reg 3.629233e+04 train accuracy: 0.302143 val accuracy: 0.316000\n",
      "lr 9.849275e-07 reg 5.189320e+04 train accuracy: 0.298653 val accuracy: 0.305000\n",
      "lr 1.129706e-06 reg 5.844782e+04 train accuracy: 0.289776 val accuracy: 0.303000\n",
      "lr 1.259727e-06 reg 1.754584e+04 train accuracy: 0.317776 val accuracy: 0.316000\n",
      "lr 1.282825e-06 reg 7.264195e+04 train accuracy: 0.258837 val accuracy: 0.266000\n",
      "lr 1.322760e-06 reg 3.934878e+04 train accuracy: 0.292388 val accuracy: 0.307000\n",
      "lr 1.438873e-06 reg 5.636387e+04 train accuracy: 0.302367 val accuracy: 0.319000\n",
      "lr 1.442018e-06 reg 6.477665e+04 train accuracy: 0.272673 val accuracy: 0.275000\n",
      "lr 1.528243e-06 reg 7.902307e+04 train accuracy: 0.270184 val accuracy: 0.278000\n",
      "lr 1.550728e-06 reg 7.131175e+04 train accuracy: 0.281163 val accuracy: 0.288000\n",
      "lr 1.586651e-06 reg 6.578545e+04 train accuracy: 0.288429 val accuracy: 0.291000\n",
      "lr 1.820539e-06 reg 2.383328e+04 train accuracy: 0.299082 val accuracy: 0.308000\n",
      "lr 1.962370e-06 reg 3.850274e+04 train accuracy: 0.281714 val accuracy: 0.281000\n",
      "lr 2.001409e-06 reg 7.798983e+04 train accuracy: 0.255898 val accuracy: 0.269000\n",
      "lr 2.174914e-06 reg 8.470816e+04 train accuracy: 0.263082 val accuracy: 0.270000\n",
      "lr 2.340618e-06 reg 7.322525e+04 train accuracy: 0.262592 val accuracy: 0.267000\n",
      "lr 2.344394e-06 reg 3.943679e+04 train accuracy: 0.301796 val accuracy: 0.301000\n",
      "lr 2.407223e-06 reg 3.864305e+04 train accuracy: 0.278367 val accuracy: 0.289000\n",
      "lr 2.712627e-06 reg 1.595074e+04 train accuracy: 0.316184 val accuracy: 0.345000\n",
      "lr 2.789726e-06 reg 5.906831e+03 train accuracy: 0.343653 val accuracy: 0.342000\n",
      "lr 2.799722e-06 reg 2.653864e+04 train accuracy: 0.291204 val accuracy: 0.304000\n",
      "lr 2.899218e-06 reg 6.974001e+03 train accuracy: 0.323388 val accuracy: 0.331000\n",
      "lr 2.928442e-06 reg 2.185621e+04 train accuracy: 0.310102 val accuracy: 0.329000\n",
      "lr 3.169202e-06 reg 9.821430e+04 train accuracy: 0.244714 val accuracy: 0.257000\n",
      "lr 3.306772e-06 reg 5.999976e+04 train accuracy: 0.227714 val accuracy: 0.222000\n",
      "lr 3.735778e-06 reg 2.249821e+04 train accuracy: 0.280878 val accuracy: 0.284000\n",
      "lr 3.737442e-06 reg 9.834707e+04 train accuracy: 0.138306 val accuracy: 0.140000\n",
      "lr 4.030653e-06 reg 9.879070e+04 train accuracy: 0.111612 val accuracy: 0.116000\n",
      "lr 4.040347e-06 reg 5.483912e+04 train accuracy: 0.169204 val accuracy: 0.161000\n",
      "lr 4.069785e-06 reg 5.486214e+04 train accuracy: 0.218694 val accuracy: 0.220000\n",
      "lr 4.087375e-06 reg 9.927302e+04 train accuracy: 0.079490 val accuracy: 0.068000\n",
      "lr 4.157747e-06 reg 2.065270e+04 train accuracy: 0.255776 val accuracy: 0.253000\n",
      "lr 4.366396e-06 reg 9.320657e+04 train accuracy: 0.132163 val accuracy: 0.126000\n",
      "lr 4.448981e-06 reg 2.419822e+04 train accuracy: 0.261837 val accuracy: 0.272000\n",
      "lr 4.639464e-06 reg 5.587983e+04 train accuracy: 0.133490 val accuracy: 0.124000\n",
      "lr 4.756741e-06 reg 4.135893e+04 train accuracy: 0.205061 val accuracy: 0.225000\n",
      "lr 4.777907e-06 reg 5.327240e+04 train accuracy: 0.140776 val accuracy: 0.157000\n",
      "lr 4.802782e-06 reg 6.402265e+04 train accuracy: 0.116939 val accuracy: 0.112000\n",
      "lr 5.008198e-06 reg 2.395148e+04 train accuracy: 0.188122 val accuracy: 0.206000\n",
      "lr 5.268511e-06 reg 4.690716e+04 train accuracy: 0.184939 val accuracy: 0.204000\n",
      "lr 5.305274e-06 reg 6.457580e+04 train accuracy: 0.106898 val accuracy: 0.111000\n",
      "lr 5.376531e-06 reg 2.931547e+04 train accuracy: 0.214694 val accuracy: 0.245000\n",
      "lr 5.448149e-06 reg 7.134468e+04 train accuracy: 0.125367 val accuracy: 0.125000\n",
      "lr 5.572491e-06 reg 3.865369e+04 train accuracy: 0.121980 val accuracy: 0.126000\n",
      "lr 5.652278e-06 reg 2.337839e+04 train accuracy: 0.190327 val accuracy: 0.198000\n",
      "lr 5.673794e-06 reg 7.753321e+04 train accuracy: 0.083653 val accuracy: 0.079000\n",
      "lr 5.708235e-06 reg 5.243797e+02 train accuracy: 0.356796 val accuracy: 0.345000\n",
      "lr 5.750296e-06 reg 1.514267e+03 train accuracy: 0.325531 val accuracy: 0.317000\n",
      "lr 5.765357e-06 reg 6.360176e+04 train accuracy: 0.123510 val accuracy: 0.114000\n",
      "lr 5.968209e-06 reg 1.280543e+03 train accuracy: 0.243102 val accuracy: 0.262000\n",
      "lr 5.985508e-06 reg 7.378198e+04 train accuracy: 0.141041 val accuracy: 0.149000\n",
      "lr 6.074584e-06 reg 6.199826e+04 train accuracy: 0.112673 val accuracy: 0.116000\n",
      "lr 6.077761e-06 reg 7.881977e+04 train accuracy: 0.141469 val accuracy: 0.147000\n",
      "lr 6.266189e-06 reg 9.013370e+04 train accuracy: 0.079367 val accuracy: 0.080000\n",
      "lr 6.319292e-06 reg 2.886807e+04 train accuracy: 0.162367 val accuracy: 0.145000\n",
      "lr 6.403824e-06 reg 4.280431e+04 train accuracy: 0.168184 val accuracy: 0.165000\n",
      "lr 6.459040e-06 reg 1.240838e+04 train accuracy: 0.223837 val accuracy: 0.214000\n",
      "lr 6.525523e-06 reg 1.384759e+04 train accuracy: 0.215265 val accuracy: 0.238000\n",
      "lr 6.865157e-06 reg 6.210051e+04 train accuracy: 0.117449 val accuracy: 0.134000\n",
      "lr 7.032191e-06 reg 2.010112e+03 train accuracy: 0.231531 val accuracy: 0.234000\n",
      "lr 7.091109e-06 reg 7.489254e+04 train accuracy: 0.071163 val accuracy: 0.080000\n",
      "lr 7.188137e-06 reg 5.741760e+04 train accuracy: 0.077143 val accuracy: 0.086000\n",
      "lr 7.188252e-06 reg 7.541893e+04 train accuracy: 0.152143 val accuracy: 0.136000\n",
      "lr 7.443983e-06 reg 8.837982e+03 train accuracy: 0.198061 val accuracy: 0.215000\n",
      "lr 7.748478e-06 reg 3.426467e+04 train accuracy: 0.125265 val accuracy: 0.128000\n",
      "lr 7.899934e-06 reg 3.333056e+04 train accuracy: 0.136286 val accuracy: 0.130000\n",
      "lr 7.910184e-06 reg 5.948230e+04 train accuracy: 0.120531 val accuracy: 0.116000\n",
      "lr 7.946271e-06 reg 9.155369e+04 train accuracy: 0.100673 val accuracy: 0.102000\n",
      "lr 8.004757e-06 reg 1.258831e+04 train accuracy: 0.196653 val accuracy: 0.208000\n",
      "lr 8.012531e-06 reg 6.369669e+04 train accuracy: 0.103673 val accuracy: 0.108000\n",
      "lr 8.062642e-06 reg 3.546042e+04 train accuracy: 0.126102 val accuracy: 0.141000\n",
      "lr 8.103056e-06 reg 2.376270e+04 train accuracy: 0.173020 val accuracy: 0.195000\n",
      "lr 8.304242e-06 reg 6.561915e+04 train accuracy: 0.149755 val accuracy: 0.137000\n",
      "lr 8.401334e-06 reg 9.767960e+04 train accuracy: 0.134020 val accuracy: 0.141000\n",
      "lr 8.424166e-06 reg 5.269305e+04 train accuracy: 0.126449 val accuracy: 0.133000\n",
      "lr 8.558156e-06 reg 4.076400e+04 train accuracy: 0.120673 val accuracy: 0.114000\n",
      "lr 8.601653e-06 reg 5.439767e+03 train accuracy: 0.207041 val accuracy: 0.205000\n",
      "lr 8.627719e-06 reg 6.186731e+04 train accuracy: 0.119061 val accuracy: 0.129000\n",
      "lr 8.629151e-06 reg 3.061098e+04 train accuracy: 0.134816 val accuracy: 0.121000\n",
      "lr 8.693498e-06 reg 7.342394e+04 train accuracy: 0.098816 val accuracy: 0.107000\n",
      "lr 8.858745e-06 reg 6.678243e+03 train accuracy: 0.206980 val accuracy: 0.212000\n",
      "lr 9.017951e-06 reg 4.987210e+04 train accuracy: 0.085653 val accuracy: 0.090000\n",
      "lr 9.028585e-06 reg 4.090189e+04 train accuracy: 0.122898 val accuracy: 0.133000\n",
      "lr 9.041059e-06 reg 9.258340e+04 train accuracy: 0.110041 val accuracy: 0.102000\n",
      "lr 9.130559e-06 reg 4.116992e+02 train accuracy: 0.276020 val accuracy: 0.296000\n",
      "lr 9.301049e-06 reg 6.720345e+04 train accuracy: 0.111714 val accuracy: 0.113000\n",
      "lr 9.728361e-06 reg 5.011213e+03 train accuracy: 0.190388 val accuracy: 0.199000\n",
      "lr 9.785866e-06 reg 1.600085e+04 train accuracy: 0.148735 val accuracy: 0.161000\n",
      "lr 9.929892e-06 reg 4.184881e+04 train accuracy: 0.145449 val accuracy: 0.147000\n",
      "best validation accuracy achieved during cross-validation: 0.353000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "sm = Softmax()\n",
    "for i in range(100):\n",
    "    lr = (1e-5 - 1e-10) * np.random.random_sample() + 1e-10\n",
    "    rs = (1e5 - 1e-2) * np.random.random_sample() + 1e-2\n",
    "    \n",
    "    print('Testing lr, rs ', (lr, rs))\n",
    "    loss_hist = sm.train(X_train, y_train, learning_rate=lr, reg=rs,\n",
    "                      num_iters=500, verbose=True)\n",
    "    train_accuracy = np.mean(y_train == sm.predict(X_train))\n",
    "    validation_accuracy = np.mean(y_val == sm.predict(X_val))\n",
    "    if best_val < validation_accuracy:\n",
    "        best_val = validation_accuracy\n",
    "        best_softmax = sm\n",
    "        best_lr = lr\n",
    "        best_rs = rs\n",
    "    results[(lr, rs)] = (train_accuracy, validation_accuracy)\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.277000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question** - *True or False*\n",
    "\n",
    "It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "*Your answer*:\n",
    "\n",
    "*True*\n",
    "\n",
    "*Your explanation*:\n",
    "\n",
    "*It is impossible because to keep the softmax loss unchange, the score for the new example have to be non-zero for the correct class and zero for all the other classes. But this is impossible be cause the softmax the weight for the wrong class is the exponential function e^x which is never zero*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
